{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlohT6mvIc_R"
      },
      "source": [
        "## Import relevant modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n9_cTveKmse",
        "outputId": "2a212aff-f74a-46c3-bc97-37af0f91c8db"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchmetrics import Accuracy, AUROC\n",
        "from tqdm import trange\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "\n",
        "print(\"Imported modules.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RZMPwe5SpJZo"
      },
      "source": [
        "## Select the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = [\"heart_disease\", \"ixi\" ,\"isic2019\"]\n",
        "dataset_selected = datasets[0]\n",
        "if dataset_selected == datasets[0]:\n",
        "    from flamby.datasets.fed_heart_disease import FedHeartDisease as FedDataset\n",
        "    from flamby.datasets.fed_heart_disease import (\n",
        "        BATCH_SIZE,\n",
        "        LR,\n",
        "        NUM_EPOCHS_POOLED,\n",
        "        BaselineLoss,\n",
        "        NUM_CLIENTS,\n",
        "        Optimizer,\n",
        "        get_nb_max_rounds\n",
        "    )\n",
        "\n",
        "elif dataset_selected == datasets[1]:\n",
        "    from flamby.datasets.fed_ixi import FedIXITiny as FedDataset\n",
        "    from flamby.datasets.fed_ixi import (\n",
        "        BATCH_SIZE,\n",
        "        LR,\n",
        "        NUM_EPOCHS_POOLED,\n",
        "        BaselineLoss,\n",
        "        NUM_CLIENTS,\n",
        "        Optimizer,\n",
        "        get_nb_max_rounds\n",
        "    )\n",
        "    def metric(output, target, epsilon=1e-9):\n",
        "        SPATIAL_DIMENSIONS = 2, 3, 4\n",
        "        p0 = output\n",
        "        g0 = target\n",
        "        p1 = 1 - p0\n",
        "        g1 = 1 - g0\n",
        "        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
        "        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "        num = 2 * tp\n",
        "        denom = 2 * tp + fp + fn + epsilon\n",
        "        dice_score = num / denom\n",
        "        return torch.mean(dice_score).item()\n",
        "\n",
        "elif dataset_selected == datasets[2]:\n",
        "    from flamby.datasets.fed_isic2019 import FedIsic2019 as FedDataset\n",
        "    from flamby.datasets.fed_isic2019 import (\n",
        "        BATCH_SIZE,\n",
        "        LR,\n",
        "        NUM_EPOCHS_POOLED,\n",
        "        BaselineLoss,\n",
        "        NUM_CLIENTS,\n",
        "        Optimizer,\n",
        "        get_nb_max_rounds\n",
        "    )\n",
        "    def metric(logits, y_true):\n",
        "        y_true = torch.reshape(y_true, (-1,))\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        return balanced_accuracy_score(y_true.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
        "\n",
        "print(\"Batch size = \", BATCH_SIZE)\n",
        "print(\"Optimizer = \", Optimizer)\n",
        "print(\"Learning rate = \", LR)\n",
        "print(\"Local iteration = \", NUM_EPOCHS_POOLED)\n",
        "print(\"Global round = \", get_nb_max_rounds(NUM_EPOCHS_POOLED))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "domb3ZbuK-2x"
      },
      "source": [
        "## Set FL training options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11nP_RwwK-le"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class option:\n",
        "  def __init__(self):\n",
        "    self.alg = 'FedAvg'  # [FedAvg | FedBN | FedProx | FedAP]\n",
        "    self.dataset = dataset_selected  # [heart_disease | ixi | isic2019]\n",
        "    self.device = 'cuda'  # [cuda | cpu]\n",
        "    self.batch = BATCH_SIZE\n",
        "    self.iters = NUM_EPOCHS_POOLED\n",
        "    self.round = get_nb_max_rounds(NUM_EPOCHS_POOLED)\n",
        "    self.optimizer = Optimizer\n",
        "    self.lr = LR\n",
        "    if dataset_selected == \"heart_disease\":\n",
        "      self.metric = Accuracy(task=\"binary\")\n",
        "    elif dataset_selected == \"ixi\":\n",
        "      self.metric = metric\n",
        "    elif dataset_selected == \"isic2019\":\n",
        "      self.metric = metric #AUROC(task=\"multiclass\", num_classes=8)\n",
        "    self.loss = BaselineLoss()\n",
        "    self.n_clients = NUM_CLIENTS\n",
        "    self.nosharebn = True if self.alg in ['FedBN', 'FedAP'] else False\n",
        "    self.mu = 1e-2 # only for FedProx\n",
        "    self.seed = 0\n",
        "\n",
        "\n",
        "args = option()\n",
        "args.random_state = np.random.RandomState(1)\n",
        "set_random_seed(args.seed)\n",
        "print(\"Strategy is: \", args.alg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load the dataset and conduct Federated Learning dataset partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZlvdpyYKx7V",
        "outputId": "e8eff65f-bd5b-4e97-fad8-6140727a4e54"
      },
      "outputs": [],
      "source": [
        "def global_test_dataset():\n",
        "    return [\n",
        "      torch.utils.data.DataLoader(\n",
        "      FedDataset(train=False, pooled=True),\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      drop_last=True\n",
        "    )\n",
        "  ]\n",
        "\n",
        "def local_val_datasets():\n",
        "  return [\n",
        "    torch.utils.data.DataLoader(\n",
        "        FedDataset(center=i, train=False, pooled=False),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        drop_last=True\n",
        "        )\n",
        "    for i in range(NUM_CLIENTS)\n",
        "  ]\n",
        "\n",
        "def local_train_datasets():\n",
        "  return [\n",
        "    torch.utils.data.DataLoader(\n",
        "        FedDataset(center=i, train=True, pooled=False),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        drop_last=True\n",
        "        )\n",
        "    for i in range(NUM_CLIENTS)\n",
        "  ]\n",
        "\n",
        "def client_datasets(cid):\n",
        "  train_loaders = local_train_datasets()\n",
        "  val_loaders = local_val_datasets()\n",
        "  return train_loaders[cid], val_loaders[cid]\n",
        "\n",
        "def server_dataset():\n",
        "  test_loader = global_test_dataset()\n",
        "  return test_loader[0]\n",
        "\n",
        "# train_loaders = local_train_datasets()\n",
        "# val_loaders = local_val_datasets()\n",
        "# test_loader = global_test_dataset()\n",
        "# test_loader = test_loader[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKapOnj594fY"
      },
      "source": [
        "## Observe size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tspgXM0494fZ",
        "outputId": "9639a303-d153-4ffa-a3e9-0ad00593346f"
      },
      "outputs": [],
      "source": [
        "# assert args.n_clients == len(train_loaders)\n",
        "\n",
        "# print(\"Number of clients: \", len(val_loaders))\n",
        "# for i, client in enumerate(val_loaders):\n",
        "#   print(\"client\", i+1, \"number of batches\" , len(client))\n",
        "#   for data in client:\n",
        "#     print(data[0].shape, data[1].shape)\n",
        "#     print(data[0], data[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jn_RT-YUGxVL"
      },
      "source": [
        "## Centralized Federated Learning using Flower framework\n",
        "## Import Flower relevant modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdxuWCrqG46S",
        "outputId": "f7ad3c0f-5db8-4fa5-ec34-7fd8d6bc3ee7"
      },
      "outputs": [],
      "source": [
        "#!pip install flwr[simulation]\n",
        "import flwr as fl\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from flwr.common.typing import Parameters\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLgvArHoIwE-"
      },
      "source": [
        "## Create neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iAMXZc-IuQx"
      },
      "outputs": [],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "\n",
        "class ConvolutionalBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dimensions: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        normalization: Optional[str] = None,\n",
        "        kernel_size: int = 3,\n",
        "        activation: Optional[str] = \"ReLU\",\n",
        "        preactivation: bool = False,\n",
        "        padding: int = 0,\n",
        "        padding_mode: str = \"zeros\",\n",
        "        dilation: Optional[int] = None,\n",
        "        dropout: float = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        block = nn.ModuleList()\n",
        "\n",
        "        dilation = 1 if dilation is None else dilation\n",
        "        if padding:\n",
        "            total_padding = kernel_size + 2 * (dilation - 1) - 1\n",
        "            padding = total_padding // 2\n",
        "\n",
        "        class_name = \"Conv{}d\".format(dimensions)\n",
        "        conv_class = getattr(nn, class_name)\n",
        "        no_bias = not preactivation and (normalization is not None)\n",
        "        conv_layer = conv_class(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=padding,\n",
        "            padding_mode=padding_mode,\n",
        "            dilation=dilation,\n",
        "            bias=not no_bias,\n",
        "        )\n",
        "\n",
        "        norm_layer = None\n",
        "        if normalization is not None:\n",
        "            class_name = \"{}Norm{}d\".format(normalization.capitalize(), dimensions)\n",
        "            norm_class = getattr(nn, class_name)\n",
        "            num_features = in_channels if preactivation else out_channels\n",
        "            norm_layer = norm_class(num_features)\n",
        "\n",
        "        activation_layer = None\n",
        "        if activation is not None:\n",
        "            activation_layer = getattr(nn, activation)()\n",
        "\n",
        "        if preactivation:\n",
        "            self.add_if_not_none(block, norm_layer)\n",
        "            self.add_if_not_none(block, activation_layer)\n",
        "            self.add_if_not_none(block, conv_layer)\n",
        "        else:\n",
        "            self.add_if_not_none(block, conv_layer)\n",
        "            self.add_if_not_none(block, norm_layer)\n",
        "            self.add_if_not_none(block, activation_layer)\n",
        "\n",
        "        dropout_layer = None\n",
        "        if dropout:\n",
        "            class_name = \"Dropout{}d\".format(dimensions)\n",
        "            dropout_class = getattr(nn, class_name)\n",
        "            dropout_layer = dropout_class(p=dropout)\n",
        "            self.add_if_not_none(block, dropout_layer)\n",
        "\n",
        "        self.conv_layer = conv_layer\n",
        "        self.norm_layer = norm_layer\n",
        "        self.activation_layer = activation_layer\n",
        "        self.dropout_layer = dropout_layer\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_if_not_none(module_list, module):\n",
        "        if module is not None:\n",
        "            module_list.append(module)\n",
        "\n",
        "\n",
        "# Decoding\n",
        "\n",
        "CHANNELS_DIMENSION = 1\n",
        "UPSAMPLING_MODES = (\"nearest\", \"linear\", \"bilinear\", \"bicubic\", \"trilinear\")\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels_skip_connection: int,\n",
        "        dimensions: int,\n",
        "        upsampling_type: str,\n",
        "        num_decoding_blocks: int,\n",
        "        normalization: Optional[str],\n",
        "        preactivation: bool = False,\n",
        "        residual: bool = False,\n",
        "        padding: int = 0,\n",
        "        padding_mode: str = \"zeros\",\n",
        "        activation: Optional[str] = \"ReLU\",\n",
        "        initial_dilation: Optional[int] = None,\n",
        "        dropout: float = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        upsampling_type = fix_upsampling_type(upsampling_type, dimensions)\n",
        "        self.decoding_blocks = nn.ModuleList()\n",
        "        self.dilation = initial_dilation\n",
        "        for _ in range(num_decoding_blocks):\n",
        "            decoding_block = DecodingBlock(\n",
        "                in_channels_skip_connection,\n",
        "                dimensions,\n",
        "                upsampling_type,\n",
        "                normalization=normalization,\n",
        "                preactivation=preactivation,\n",
        "                residual=residual,\n",
        "                padding=padding,\n",
        "                padding_mode=padding_mode,\n",
        "                activation=activation,\n",
        "                dilation=self.dilation,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            self.decoding_blocks.append(decoding_block)\n",
        "            in_channels_skip_connection //= 2\n",
        "            if self.dilation is not None:\n",
        "                self.dilation //= 2\n",
        "\n",
        "    def forward(self, skip_connections, x):\n",
        "        zipped = zip(reversed(skip_connections), self.decoding_blocks)\n",
        "        for skip_connection, decoding_block in zipped:\n",
        "            x = decoding_block(skip_connection, x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecodingBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels_skip_connection: int,\n",
        "        dimensions: int,\n",
        "        upsampling_type: str,\n",
        "        normalization: Optional[str],\n",
        "        preactivation: bool = True,\n",
        "        residual: bool = False,\n",
        "        padding: int = 0,\n",
        "        padding_mode: str = \"zeros\",\n",
        "        activation: Optional[str] = \"ReLU\",\n",
        "        dilation: Optional[int] = None,\n",
        "        dropout: float = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual = residual\n",
        "\n",
        "        if upsampling_type == \"conv\":\n",
        "            in_channels = out_channels = 2 * in_channels_skip_connection\n",
        "            self.upsample = get_conv_transpose_layer(\n",
        "                dimensions, in_channels, out_channels\n",
        "            )\n",
        "        else:\n",
        "            self.upsample = get_upsampling_layer(upsampling_type)\n",
        "        in_channels_first = in_channels_skip_connection * (1 + 2)\n",
        "        out_channels = in_channels_skip_connection\n",
        "        self.conv1 = ConvolutionalBlock(\n",
        "            dimensions,\n",
        "            in_channels_first,\n",
        "            out_channels,\n",
        "            normalization=normalization,\n",
        "            preactivation=preactivation,\n",
        "            padding=padding,\n",
        "            padding_mode=padding_mode,\n",
        "            activation=activation,\n",
        "            dilation=dilation,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        in_channels_second = out_channels\n",
        "        self.conv2 = ConvolutionalBlock(\n",
        "            dimensions,\n",
        "            in_channels_second,\n",
        "            out_channels,\n",
        "            normalization=normalization,\n",
        "            preactivation=preactivation,\n",
        "            padding=padding,\n",
        "            padding_mode=padding_mode,\n",
        "            activation=activation,\n",
        "            dilation=dilation,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        if residual:\n",
        "            self.conv_residual = ConvolutionalBlock(\n",
        "                dimensions,\n",
        "                in_channels_first,\n",
        "                out_channels,\n",
        "                kernel_size=1,\n",
        "                normalization=None,\n",
        "                activation=None,\n",
        "            )\n",
        "\n",
        "    def forward(self, skip_connection, x):\n",
        "        x = self.upsample(x)\n",
        "        skip_connection = self.center_crop(skip_connection, x)\n",
        "        x = torch.cat((skip_connection, x), dim=CHANNELS_DIMENSION)\n",
        "        if self.residual:\n",
        "            connection = self.conv_residual(x)\n",
        "            x = self.conv1(x)\n",
        "            x = self.conv2(x)\n",
        "            x += connection\n",
        "        else:\n",
        "            x = self.conv1(x)\n",
        "            x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "    def center_crop(self, skip_connection, x):\n",
        "        skip_shape = torch.tensor(skip_connection.shape)\n",
        "        x_shape = torch.tensor(x.shape)\n",
        "        crop = skip_shape[2:] - x_shape[2:]\n",
        "        half_crop = crop // 2\n",
        "        # If skip_connection is 10, 20, 30 and x is (6, 14, 12)\n",
        "        # Then pad will be (-2, -2, -3, -3, -9, -9)\n",
        "        pad = -torch.stack((half_crop, half_crop)).t().flatten()\n",
        "        skip_connection = F.pad(skip_connection, pad.tolist())\n",
        "        return skip_connection\n",
        "\n",
        "\n",
        "def get_upsampling_layer(upsampling_type: str) -> nn.Upsample:\n",
        "    if upsampling_type not in UPSAMPLING_MODES:\n",
        "        message = 'Upsampling type is \"{}\"' \" but should be one of the following: {}\"\n",
        "        message = message.format(upsampling_type, UPSAMPLING_MODES)\n",
        "        raise ValueError(message)\n",
        "    upsample = nn.Upsample(scale_factor=2, mode=upsampling_type, align_corners=False)\n",
        "    return upsample\n",
        "\n",
        "\n",
        "def get_conv_transpose_layer(dimensions, in_channels, out_channels):\n",
        "    class_name = \"ConvTranspose{}d\".format(dimensions)\n",
        "    conv_class = getattr(nn, class_name)\n",
        "    conv_layer = conv_class(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "    return conv_layer\n",
        "\n",
        "\n",
        "def fix_upsampling_type(upsampling_type: str, dimensions: int):\n",
        "    if upsampling_type == \"linear\":\n",
        "        if dimensions == 2:\n",
        "            upsampling_type = \"bilinear\"\n",
        "        elif dimensions == 3:\n",
        "            upsampling_type = \"trilinear\"\n",
        "    return upsampling_type\n",
        "\n",
        "\n",
        "# Encoding\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels_first: int,\n",
        "        dimensions: int,\n",
        "        pooling_type: str,\n",
        "        num_encoding_blocks: int,\n",
        "        normalization: Optional[str],\n",
        "        preactivation: bool = False,\n",
        "        residual: bool = False,\n",
        "        padding: int = 0,\n",
        "        padding_mode: str = \"zeros\",\n",
        "        activation: Optional[str] = \"ReLU\",\n",
        "        initial_dilation: Optional[int] = None,\n",
        "        dropout: float = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoding_blocks = nn.ModuleList()\n",
        "        self.dilation = initial_dilation\n",
        "        is_first_block = True\n",
        "        for _ in range(num_encoding_blocks):\n",
        "            encoding_block = EncodingBlock(\n",
        "                in_channels,\n",
        "                out_channels_first,\n",
        "                dimensions,\n",
        "                normalization,\n",
        "                pooling_type,\n",
        "                preactivation,\n",
        "                is_first_block=is_first_block,\n",
        "                residual=residual,\n",
        "                padding=padding,\n",
        "                padding_mode=padding_mode,\n",
        "                activation=activation,\n",
        "                dilation=self.dilation,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            is_first_block = False\n",
        "            self.encoding_blocks.append(encoding_block)\n",
        "            if dimensions == 2:\n",
        "                in_channels = out_channels_first\n",
        "                out_channels_first = in_channels * 2\n",
        "            elif dimensions == 3:\n",
        "                in_channels = 2 * out_channels_first\n",
        "                out_channels_first = in_channels\n",
        "            if self.dilation is not None:\n",
        "                self.dilation *= 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for encoding_block in self.encoding_blocks:\n",
        "            x, skip_connnection = encoding_block(x)\n",
        "            skip_connections.append(skip_connnection)\n",
        "        return skip_connections, x\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return self.encoding_blocks[-1].out_channels\n",
        "\n",
        "\n",
        "class EncodingBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels_first: int,\n",
        "        dimensions: int,\n",
        "        normalization: Optional[str],\n",
        "        pooling_type: Optional[str],\n",
        "        preactivation: bool = False,\n",
        "        is_first_block: bool = False,\n",
        "        residual: bool = False,\n",
        "        padding: int = 0,\n",
        "        padding_mode: str = \"zeros\",\n",
        "        activation: Optional[str] = \"ReLU\",\n",
        "        dilation: Optional[int] = None,\n",
        "        dropout: float = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.preactivation = preactivation\n",
        "        self.normalization = normalization\n",
        "\n",
        "        self.residual = residual\n",
        "\n",
        "        if is_first_block:\n",
        "            normalization = None\n",
        "            preactivation = None\n",
        "        else:\n",
        "            normalization = self.normalization\n",
        "            preactivation = self.preactivation\n",
        "\n",
        "        self.conv1 = ConvolutionalBlock(\n",
        "            dimensions,\n",
        "            in_channels,\n",
        "            out_channels_first,\n",
        "            normalization=normalization,\n",
        "            preactivation=preactivation,\n",
        "            padding=padding,\n",
        "            padding_mode=padding_mode,\n",
        "            activation=activation,\n",
        "            dilation=dilation,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        if dimensions == 2:\n",
        "            out_channels_second = out_channels_first\n",
        "        elif dimensions == 3:\n",
        "            out_channels_second = 2 * out_channels_first\n",
        "        self.conv2 = ConvolutionalBlock(\n",
        "            dimensions,\n",
        "            out_channels_first,\n",
        "            out_channels_second,\n",
        "            normalization=self.normalization,\n",
        "            preactivation=self.preactivation,\n",
        "            padding=padding,\n",
        "            activation=activation,\n",
        "            dilation=dilation,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        if residual:\n",
        "            self.conv_residual = ConvolutionalBlock(\n",
        "                dimensions,\n",
        "                in_channels,\n",
        "                out_channels_second,\n",
        "                kernel_size=1,\n",
        "                normalization=None,\n",
        "                activation=None,\n",
        "            )\n",
        "\n",
        "        self.downsample = None\n",
        "        if pooling_type is not None:\n",
        "            self.downsample = get_downsampling_layer(dimensions, pooling_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            connection = self.conv_residual(x)\n",
        "            x = self.conv1(x)\n",
        "            x = self.conv2(x)\n",
        "            x += connection\n",
        "        else:\n",
        "            x = self.conv1(x)\n",
        "            x = self.conv2(x)\n",
        "        if self.downsample is None:\n",
        "            return x\n",
        "        else:\n",
        "            skip_connection = x\n",
        "            x = self.downsample(x)\n",
        "            return x, skip_connection\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return self.conv2.conv_layer.out_channels\n",
        "\n",
        "\n",
        "def get_downsampling_layer(\n",
        "    dimensions: int, pooling_type: str, kernel_size: int = 2\n",
        ") -> nn.Module:\n",
        "    class_name = \"{}Pool{}d\".format(pooling_type.capitalize(), dimensions)\n",
        "    class_ = getattr(nn, class_name)\n",
        "    return class_(kernel_size)\n",
        "\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "    if args.dataset == datasets[0]:\n",
        "        def __init__(self, input_dim=13, output_dim=1):\n",
        "            super(Baseline, self).__init__()\n",
        "            self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "            self.linear1 = torch.nn.Linear(input_dim, 32)\n",
        "            self.bn = torch.nn.BatchNorm1d(32)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.linear2 = torch.nn.Linear(32, output_dim)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            x = self.linear1(x)\n",
        "            x = self.relu(self.bn(x))\n",
        "            x = self.linear2(x)\n",
        "            return torch.sigmoid(x)\n",
        "            # return torch.sigmoid(self.linear(x))\n",
        "\n",
        "    elif args.dataset == datasets[1]:\n",
        "        def __init__(\n",
        "            self,\n",
        "            in_channels: int = 1,\n",
        "            out_classes: int = 2,\n",
        "            dimensions: int = 3,\n",
        "            num_encoding_blocks: int = 3,\n",
        "            out_channels_first_layer: int = 8,\n",
        "            normalization: Optional[str] = \"batch\",\n",
        "            pooling_type: str = \"max\",\n",
        "            upsampling_type: str = \"linear\",\n",
        "            preactivation: bool = False,\n",
        "            residual: bool = False,\n",
        "            padding: int = 1,\n",
        "            padding_mode: str = \"zeros\",\n",
        "            activation: Optional[str] = \"PReLU\",\n",
        "            initial_dilation: Optional[int] = None,\n",
        "            dropout: float = 0,\n",
        "            monte_carlo_dropout: float = 0,\n",
        "        ):\n",
        "            super().__init__()\n",
        "            self.CHANNELS_DIMENSION = 1\n",
        "            depth = num_encoding_blocks - 1\n",
        "\n",
        "            # Force padding if residual blocks\n",
        "            if residual:\n",
        "                padding = 1\n",
        "\n",
        "            # Encoder\n",
        "            self.encoder = Encoder(\n",
        "                in_channels,\n",
        "                out_channels_first_layer,\n",
        "                dimensions,\n",
        "                pooling_type,\n",
        "                depth,\n",
        "                normalization,\n",
        "                preactivation=preactivation,\n",
        "                residual=residual,\n",
        "                padding=padding,\n",
        "                padding_mode=padding_mode,\n",
        "                activation=activation,\n",
        "                initial_dilation=initial_dilation,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "\n",
        "            # Bottom (last encoding block)\n",
        "            in_channels = self.encoder.out_channels\n",
        "            if dimensions == 2:\n",
        "                out_channels_first = 2 * in_channels\n",
        "            else:\n",
        "                out_channels_first = in_channels\n",
        "\n",
        "            self.bottom_block = EncodingBlock(\n",
        "                in_channels,\n",
        "                out_channels_first,\n",
        "                dimensions,\n",
        "                normalization,\n",
        "                pooling_type=None,\n",
        "                preactivation=preactivation,\n",
        "                residual=residual,\n",
        "                padding=padding,\n",
        "                padding_mode=padding_mode,\n",
        "                activation=activation,\n",
        "                dilation=self.encoder.dilation,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "\n",
        "            # Decoder\n",
        "            if dimensions == 2:\n",
        "                power = depth - 1\n",
        "            elif dimensions == 3:\n",
        "                power = depth\n",
        "            in_channels = self.bottom_block.out_channels\n",
        "            in_channels_skip_connection = out_channels_first_layer * 2**power\n",
        "            num_decoding_blocks = depth\n",
        "            self.decoder = Decoder(\n",
        "                in_channels_skip_connection,\n",
        "                dimensions,\n",
        "                upsampling_type,\n",
        "                num_decoding_blocks,\n",
        "                normalization=normalization,\n",
        "                preactivation=preactivation,\n",
        "                residual=residual,\n",
        "                padding=padding,\n",
        "                padding_mode=padding_mode,\n",
        "                activation=activation,\n",
        "                initial_dilation=self.encoder.dilation,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "\n",
        "            # Monte Carlo dropout\n",
        "            self.monte_carlo_layer = None\n",
        "            if monte_carlo_dropout:\n",
        "                dropout_class = getattr(nn, \"Dropout{}d\".format(dimensions))\n",
        "                self.monte_carlo_layer = dropout_class(p=monte_carlo_dropout)\n",
        "\n",
        "            # Classifier\n",
        "            if dimensions == 2:\n",
        "                in_channels = out_channels_first_layer\n",
        "            elif dimensions == 3:\n",
        "                in_channels = 2 * out_channels_first_layer\n",
        "            self.classifier = ConvolutionalBlock(\n",
        "                dimensions, in_channels, out_classes, kernel_size=1, activation=None\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            skip_connections, encoding = self.encoder(x)\n",
        "            encoding = self.bottom_block(encoding)\n",
        "            x = self.decoder(skip_connections, encoding)\n",
        "            if self.monte_carlo_layer is not None:\n",
        "                x = self.monte_carlo_layer(x)\n",
        "            x = self.classifier(x)\n",
        "            x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n",
        "            return x\n",
        "\n",
        "    elif args.dataset == datasets[2]:\n",
        "        \"\"\"Baseline model\n",
        "        We use the EfficientNets architecture that many participants in the ISIC\n",
        "        competition have identified to work best.\n",
        "        See here the [reference paper](https://arxiv.org/abs/1905.11946)\n",
        "        Thank you to [Luke Melas-Kyriazi](https://github.com/lukemelas) for his\n",
        "        [pytorch reimplementation of EfficientNets]\n",
        "        (https://github.com/lukemelas/EfficientNet-PyTorch).\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, pretrained=False, arch_name=\"efficientnet-b0\"):\n",
        "            super(Baseline, self).__init__()\n",
        "            self.pretrained = pretrained\n",
        "            self.base_model = (\n",
        "                EfficientNet.from_pretrained(arch_name)\n",
        "                if pretrained\n",
        "                else EfficientNet.from_name(arch_name)\n",
        "            )\n",
        "            # self.base_model=torchvision.models.efficientnet_v2_s(pretrained=pretrained)\n",
        "            nftrs = self.base_model._fc.in_features\n",
        "            print(\"Number of features output by EfficientNet\", nftrs)\n",
        "            self.base_model._fc = nn.Linear(nftrs, 8)\n",
        "\n",
        "        def forward(self, image):\n",
        "            out = self.base_model(image)\n",
        "            return out\n",
        "\n",
        "    def get_weights(self) -> fl.common.NDArrays:\n",
        "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
        "        if args.nosharebn == True:\n",
        "          return [val.cpu().numpy() for name, val in self.state_dict().items() if (('bn' not in name) or ('nor' not in name))]\n",
        "        else:\n",
        "          return [val.cpu().numpy() for _, val in self.state_dict().items()]\n",
        "\n",
        "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
        "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "        if args.nosharebn == True:\n",
        "          keys = [k for k in self.state_dict().keys() if (('bn' not in k) or ('nor' not in k))]\n",
        "          params_dict = zip(keys, weights)\n",
        "          state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "          self.load_state_dict(state_dict, strict=False)\n",
        "        else:\n",
        "          layer_dict = {}\n",
        "          for k, v in zip(self.state_dict().keys(), weights):\n",
        "            if v.ndim != 0:\n",
        "              layer_dict[k] = torch.Tensor(v)\n",
        "          state_dict = OrderedDict(layer_dict)\n",
        "          self.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(\n",
        "    net: Baseline,\n",
        "    trainloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_iterations: int,\n",
        "    proximal_term: float,\n",
        "    log_progress: bool = True):\n",
        "    # Define loss and optimizer\n",
        "    criterion = args.loss\n",
        "    optimizer = args.optimizer(net.parameters(), lr=args.lr)\n",
        "\n",
        "    def cycle(iterable):\n",
        "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
        "        while True:\n",
        "            for x in iterable:\n",
        "                yield x\n",
        "\n",
        "    # Train the network\n",
        "    net.train()\n",
        "    total_loss, total_correct, n_samples = 0.0, 0.0, 0\n",
        "    pbar = tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f'TRAIN') if log_progress else iter(cycle(trainloader))\n",
        "\n",
        "    # Unusually, this training is formulated in terms of number of updates/iterations/batches processed\n",
        "    # by the network. This will be helpful later on, when partitioning the data across clients: resulting\n",
        "    # in differences between dataset sizes and hence inconsistent numbers of updates per 'epoch'.\n",
        "    for i, data in zip(range(num_iterations), pbar):\n",
        "        outputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(outputs)\n",
        "        if args.alg == \"FedProx\":\n",
        "            criterion_2 = nn.Identity()\n",
        "            loss = criterion(outputs, labels) + criterion_2(proximal_term)\n",
        "            #print(\"Total loss = \", loss)\n",
        "            #print(\"Single loss = \", criterion(outputs, labels))\n",
        "        else:\n",
        "            loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collected training loss and accuracy statistics\n",
        "        total_loss += loss.item()\n",
        "        acc = args.metric(outputs, labels.type(torch.int))\n",
        "        n_samples += labels.size(0)\n",
        "        total_correct += acc * labels.size(0)\n",
        "\n",
        "        if log_progress:\n",
        "            pbar.set_postfix({\n",
        "                \"train_loss\": total_loss/n_samples, \n",
        "                \"train_acc\": total_correct/n_samples\n",
        "            })\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    return total_loss/n_samples, total_correct/n_samples, n_samples   \n",
        "  \n",
        "\n",
        "def test(\n",
        "    net: Baseline,\n",
        "    testloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    log_progress: bool = True):\n",
        "    \"\"\"Evaluates the network on test data.\"\"\"\n",
        "    criterion = args.loss\n",
        "    total_loss, total_correct, n_samples = 0.0, 0.0, 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
        "        for data in pbar:\n",
        "            outputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(outputs)\n",
        "\n",
        "            # Collected testing loss and accuracy statistics\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            acc = args.metric(outputs, labels.type(torch.int))\n",
        "            n_samples += labels.size(0)\n",
        "            total_correct += acc * labels.size(0)\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    return total_loss/n_samples, total_correct/n_samples, n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMDLObtRiB-"
      },
      "source": [
        "## Create Flower custom client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tgnk7JnRhTt"
      },
      "outputs": [],
      "source": [
        "from flwr.common import EvaluateIns, EvaluateRes, FitIns, FitRes, GetPropertiesIns, GetPropertiesRes, GetParametersIns, \\\n",
        "    GetParametersRes, Status, Code, parameters_to_ndarrays, ndarrays_to_parameters, NDArrays\n",
        "\n",
        "\n",
        "class FL_Client(fl.client.Client):\n",
        "    def __init__(self, args, client_datasets, cid: str, log_progress: bool = False):\n",
        "        self.cid = cid\n",
        "        self.args = args\n",
        "        self.client_datasets = client_datasets\n",
        "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "        self.log_progress = log_progress\n",
        "\n",
        "        # instantiate model\n",
        "        self.net = Baseline()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_properties(self, ins: GetPropertiesIns):\n",
        "        return GetPropertiesRes(properties=self.properties)\n",
        "\n",
        "    def get_parameters(self, ins: GetParametersIns):\n",
        "        return GetParametersRes(status=Status(Code.OK, \"\"), parameters=ndarrays_to_parameters(self.net.get_weights()))\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        if args.alg == \"FedAP\":\n",
        "            if isinstance(parameters, Parameters):\n",
        "                self.net.set_weights(parameters_to_ndarrays(parameters))\n",
        "                return parameters_to_ndarrays(parameters)\n",
        "            else:\n",
        "                self.net.set_weights(parameters_to_ndarrays(parameters[int(self.cid)]))\n",
        "                return parameters_to_ndarrays(parameters[int(self.cid)])\n",
        "        else:\n",
        "            self.net.set_weights(parameters_to_ndarrays(parameters))\n",
        "            return parameters_to_ndarrays(parameters)\n",
        "\n",
        "    def fit(self, fit_params: FitIns) -> FitRes:\n",
        "        # Process incoming request to train\n",
        "        trainloader, _ = self.client_datasets(int(self.cid))\n",
        "        num_iterations = fit_params.config[\"num_iterations\"]\n",
        "        global_parameters = self.set_parameters(fit_params.parameters)\n",
        "        \n",
        "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
        "        num_iterations = num_iterations or len(trainloader)\n",
        "\n",
        "        # Train the model\n",
        "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
        "        self.net.to(self.device)\n",
        "        train_loss, train_acc, num_examples = train(self.net, trainloader, device=self.device, \\\n",
        "            num_iterations=num_iterations, proximal_term=0.0, log_progress=self.log_progress)\n",
        "\n",
        "        # Proximal term calculation for FedProx strategy and re-train\n",
        "        if self.args.alg == 'FedProx':\n",
        "            local_parameters = parameters_to_ndarrays(self.get_parameters(fit_params.config).parameters)\n",
        "            proximal_term = 0.0\n",
        "            for w, w_t in zip(local_parameters, global_parameters):\n",
        "                proximal_term += args.mu / 2 * np.sqrt(np.sum(np.power((w - w_t), 2)))\n",
        "            #print(\"Proximal term = \", proximal_term)\n",
        "            \n",
        "            self.net.to(\"cpu\")\n",
        "            self.set_parameters(fit_params.parameters)\n",
        "            self.net.to(self.device)\n",
        "            train_loss, train_acc, num_examples = train(self.net, trainloader, device=self.device, \\\n",
        "                num_iterations=num_iterations, proximal_term=proximal_term, log_progress=self.log_progress)\n",
        "        \n",
        "        print(f\"Client {self.cid}: training round complete, {num_examples} examples processed\")\n",
        "\n",
        "        # Return training information: model, number of examples processed and metrics\n",
        "        return FitRes(\n",
        "            status=Status(Code.OK, \"\"),\n",
        "            parameters=self.get_parameters(fit_params.config).parameters, \n",
        "            num_examples=num_examples, \n",
        "            metrics={\"loss\": train_loss, \"accuracy\": train_acc})\n",
        "\n",
        "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
        "        # Process incoming request to evaluate\n",
        "        self.set_parameters(eval_params.parameters)\n",
        "\n",
        "        # Evaluate the model\n",
        "        self.net.to(self.device)\n",
        "        _, valloader = self.client_datasets(int(self.cid))\n",
        "        loss, accuracy, num_examples = test(self.net, valloader, device=self.device, log_progress=self.log_progress)\n",
        "\n",
        "        print(f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, accuracy={accuracy:.4f}\")\n",
        "        # Return evaluation information\n",
        "        return EvaluateRes(\n",
        "            status=Status(Code.OK, \"\"),\n",
        "            loss=loss, num_examples=num_examples, \n",
        "            metrics={\"accuracy\": accuracy})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HLvMDfgWZss"
      },
      "source": [
        "## Create server-side evaluation and experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swby9oVIWny_"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from flwr.server.strategy import FedAvg, FedAP\n",
        "from flwr.server.app import ServerConfig\n",
        "\n",
        "\n",
        "def print_model_layers(model):\n",
        "    print(model)\n",
        "    for param_tensor in model.state_dict():\n",
        "      print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "\n",
        "def serverside_eval(server_round, parameters: NDArrays, config, server_dataset):\n",
        "    \"\"\"An evaluation function for centralized/serverside evaluation over the entire test set.\"\"\"\n",
        "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = \"cpu\"\n",
        "    model = Baseline()\n",
        "    #print_model_layers(model)\n",
        "\n",
        "    model.set_weights(parameters)\n",
        "    model.to(device)\n",
        "\n",
        "    testloader = server_dataset()\n",
        "    loss, accuracy, n_samples = test(model, testloader, device=device, log_progress=False)\n",
        "\n",
        "    print(f\"Evaluation on the server: test_loss={loss:.4f}, test_accuracy={accuracy:.4f}\")\n",
        "    return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "def start_experiment(\n",
        "    args,\n",
        "    client_datasets,\n",
        "    server_dataset,\n",
        "    num_rounds=args.round, \n",
        "    alg=args.alg,\n",
        "    client_pool_size=args.n_clients, \n",
        "    num_iterations=args.iters, \n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=1,\n",
        "    batch_size=args.batch):\n",
        "    client_resources = {\"num_cpus\": 0.5}  # 2 clients per CPU\n",
        "\n",
        "    # Configure the strategy\n",
        "    def fit_config(server_round: int):\n",
        "        print(f\"Configuring round {server_round}\")\n",
        "        return {\n",
        "            \"num_iterations\": num_iterations,\n",
        "            \"batch_size\": batch_size,\n",
        "        }\n",
        "\n",
        "    # Strategy selection\n",
        "    if alg in ['FedAvg', 'FedBN', 'FedProx']:\n",
        "      strategy = FedAvg(\n",
        "          fraction_fit=fraction_fit,\n",
        "          fraction_evaluate=fraction_fit,\n",
        "          min_fit_clients=min_fit_clients,\n",
        "          min_evaluate_clients=min_fit_clients,\n",
        "          min_available_clients=client_pool_size,  # all clients should be available\n",
        "          on_fit_config_fn=fit_config,\n",
        "          on_evaluate_config_fn=(lambda r: {\"batch_size\": 100}),\n",
        "          evaluate_fn=functools.partial(serverside_eval, server_dataset=server_dataset),\n",
        "          accept_failures=False,\n",
        "      )\n",
        "    elif alg == 'FedAP':\n",
        "      strategy = FedAP(\n",
        "          fraction_fit=fraction_fit,\n",
        "          fraction_evaluate=fraction_fit,\n",
        "          min_fit_clients=min_fit_clients,\n",
        "          min_evaluate_clients=min_fit_clients,\n",
        "          min_available_clients=client_pool_size,  # all clients should be available\n",
        "          on_fit_config_fn=fit_config,\n",
        "          on_evaluate_config_fn=(lambda r: {\"batch_size\": 100}),\n",
        "          evaluate_fn=functools.partial(serverside_eval, server_dataset=server_dataset),\n",
        "          accept_failures=False,\n",
        "      )\n",
        "\n",
        "    print(f\"FL experiment configured for {num_rounds} rounds with {client_pool_size} client in the pool.\")\n",
        "    print(f\"FL round will proceed with {fraction_fit * 100}% of clients sampled, at least {min_fit_clients}.\")\n",
        "\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        \"\"\"Creates a federated learning client\"\"\"\n",
        "        return FL_Client(args, client_datasets, cid, log_progress=False)\n",
        "\n",
        "\n",
        "    # Start the simulation\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=client_pool_size,\n",
        "        client_resources=client_resources,\n",
        "        config=ServerConfig(num_rounds=num_rounds), strategy=strategy)\n",
        "    \n",
        "    print(history)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgqbxjGjfrPY"
      },
      "source": [
        "## Start federated training and inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p2eKW-bCfotZ",
        "outputId": "bd73787b-3838-4e94-a186-4903620a60dc"
      },
      "outputs": [],
      "source": [
        "start_experiment(args=args, client_datasets=client_datasets, server_dataset=server_dataset, \\\n",
        "                 fraction_fit=1.0, min_fit_clients=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "flamby",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e2855e6afe31ce834df930670fa09e4bc6f549fe2f635bc411013f58e2d5fda"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
